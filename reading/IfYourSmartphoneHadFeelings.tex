This is a call to action by the author to take the possibility of AI having a consciousness/subjective experience/feelings more seriously.
They make the argument that there really is no proof that a consciousness must emerge from a carbon-based organism.
They also claim that humans tend to exclude other forms of living organisms from what he calls the ``moral circle,'' or the types of beings that are considered worthy of care and ethical consideration.
One main concern of the author is that if we keep neglecting the very possibility of AI becoming sentient, we could end up in a scenario where these ``digital animals'' (as he calls them) could have endured much suffering at the hands of humans.

I get where he's coming from, but I have to heavily disagree with this man.
To me, the simplest explanation about this is that Brian is scared and doesn't really understand what is going on.
I looked him up, and he writes op-eds pretty consistently.
I have the impression that these op-ed writers want to get the reader's attention at all costs, without much regard to what they're saying.
Bro also wrote this at 3am on January 2nd, so we might also want to call into question whether he was in the best mental state for writing something that we should take seriously around that time.
Also, his arguments boil down to ``well, you can't prove AI will \textit{not} be conscious anytime soon,'' which is not a valid method of justification, and I cannot really take seriously.

\textbf{Question:}
Brian's argument boils down to:
1. If AI is conscious, then we should take more precautions and ethical considerations regarding AI.
2. We cannot disprove that AI will become conscious in the near future.
3. Therefore, we should take more precautions and ethical considerations regarding AI.
I think the issue with this argument is in point 2.
So I call into question: how valid of a justification is ``we cannot disprove it''?

