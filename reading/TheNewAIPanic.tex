\textbf{Question:}
To what extent do the AI companies believe their own narratives regarding future threats of AI?
From an outside perspective, we can come to the conclusion that they are simply raising alarm bells to divert attention away from the harm they are doing today.
So, the way I see it, we are left with the option that either these people are doing this sort of fear-mongering with a malicious intent, or that they are truly worried about the dangers of the future.
I am more inclined to think that they are malicious, but then I wonder why there is actual monetary investment and research in AI safety and other such topics?
Why would these big companies invest any resources into something that they know is likely not going to happen, especially when they don't seem to care about the \textit{current} problems that their technology brings about?
Or is this investment into future concerns also a necessary step in their plan for diverting attention away from the real, immediate harm that these companies are causing?
If so, why not just spend the resources fixing the problems of today?